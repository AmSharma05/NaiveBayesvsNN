{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+inPAKxpziqJ4qa2gSF0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmSharma05/NaiveBayesvsNN/blob/main/SpamNNfinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Keras-Preprocessing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xq-ud2Fv5eG",
        "outputId": "347d096e-0f42-43bc-9cf1-273b5535e2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.15.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras.layers import Bidirectional\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(\"spam.csv\", encoding=\"'latin'\")\n",
        "data[\"text\"] = data.v2\n",
        "data[\"spam\"] = data.v1\n",
        "\n",
        "# Split data into training and testing sets\n",
        "emails_train, emails_test, target_train, target_test = train_test_split(data.text, data.spam, test_size=0.2)\n",
        "\n",
        "# Define text cleaning functions\n",
        "def remove_hyperlink(word):\n",
        "    return re.sub(r\"http\\S+\", \"\", word)\n",
        "\n",
        "def to_lower(word):\n",
        "    return word.lower()\n",
        "\n",
        "def remove_number(word):\n",
        "    return re.sub(r'\\d+', '', word)\n",
        "\n",
        "def remove_punctuation(word):\n",
        "    return word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "\n",
        "def remove_whitespace(word):\n",
        "    return word.strip()\n",
        "\n",
        "def replace_newline(word):\n",
        "    return word.replace('\\n', '')\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    cleaning_utils = [remove_hyperlink, replace_newline, to_lower, remove_number, remove_punctuation, remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "    return sentence\n",
        "\n",
        "# Clean up training and testing data\n",
        "x_train = [clean_up_pipeline(o) for o in emails_train]\n",
        "x_test = [clean_up_pipeline(o) for o in emails_test]\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "train_y = le.fit_transform(target_train.values)\n",
        "test_y = le.transform(target_test.values)\n",
        "\n",
        "# Define model parameters\n",
        "embed_size = 100 # how big is each word vector\n",
        "max_feature = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
        "max_len = 2000 # max number of words in a question to use\n",
        "\n",
        "# Tokenize text data\n",
        "tokenizer = Tokenizer(num_words=max_feature)\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_features = np.array(tokenizer.texts_to_sequences(x_train))\n",
        "x_test_features = np.array(tokenizer.texts_to_sequences(x_test))\n",
        "\n",
        "# Pad sequences\n",
        "x_train_features = pad_sequences(x_train_features, maxlen=max_len)\n",
        "x_test_features = pad_sequences(x_test_features, maxlen=max_len)\n",
        "\n",
        "# Build model\n",
        "embedding_vecor_length = 32\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Embedding(max_feature, embedding_vecor_length, input_length=max_len))\n",
        "model.add(Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(x_train_features, train_y, batch_size=512, epochs=20, validation_data=(x_test_features, test_y))\n",
        "\n",
        "# Evaluate model performance\n",
        "pred_y = model.predict(x_test_features)\n",
        "pred_y = np.round(pred_y.flatten())\n",
        "\n",
        "precision = precision_score(test_y, pred_y)\n",
        "recall = recall_score(test_y, pred_y)\n",
        "f1 = f1_score(test_y, pred_y)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJlCcDkkwBuq",
        "outputId": "67c7c825-8270-48ba-de32-6f663cd8b024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-961ba4965caa>:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  x_train_features = np.array(tokenizer.texts_to_sequences(x_train))\n",
            "<ipython-input-7-961ba4965caa>:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  x_test_features = np.array(tokenizer.texts_to_sequences(x_test))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "9/9 [==============================] - 161s 17s/step - loss: 0.6533 - accuracy: 0.8281 - val_loss: 0.5756 - val_accuracy: 0.8592\n",
            "Epoch 2/20\n",
            "9/9 [==============================] - 163s 18s/step - loss: 0.4618 - accuracy: 0.8676 - val_loss: 0.4143 - val_accuracy: 0.8592\n",
            "Epoch 3/20\n",
            "9/9 [==============================] - 161s 18s/step - loss: 0.3662 - accuracy: 0.8676 - val_loss: 0.3650 - val_accuracy: 0.8592\n",
            "Epoch 4/20\n",
            "9/9 [==============================] - 163s 18s/step - loss: 0.3271 - accuracy: 0.8676 - val_loss: 0.2999 - val_accuracy: 0.8592\n",
            "Epoch 5/20\n",
            "9/9 [==============================] - 172s 19s/step - loss: 0.2309 - accuracy: 0.8840 - val_loss: 0.1901 - val_accuracy: 0.9426\n",
            "Epoch 6/20\n",
            "9/9 [==============================] - 160s 18s/step - loss: 0.1472 - accuracy: 0.9690 - val_loss: 0.1498 - val_accuracy: 0.9641\n",
            "Epoch 7/20\n",
            "9/9 [==============================] - 161s 18s/step - loss: 0.1006 - accuracy: 0.9823 - val_loss: 0.1016 - val_accuracy: 0.9731\n",
            "Epoch 8/20\n",
            "9/9 [==============================] - 160s 18s/step - loss: 0.0622 - accuracy: 0.9879 - val_loss: 0.0661 - val_accuracy: 0.9865\n",
            "Epoch 9/20\n",
            "9/9 [==============================] - 160s 18s/step - loss: 0.0327 - accuracy: 0.9910 - val_loss: 0.0538 - val_accuracy: 0.9892\n",
            "Epoch 10/20\n",
            "9/9 [==============================] - 159s 18s/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 0.0520 - val_accuracy: 0.9874\n",
            "Epoch 11/20\n",
            "9/9 [==============================] - 163s 18s/step - loss: 0.0120 - accuracy: 0.9975 - val_loss: 0.0578 - val_accuracy: 0.9848\n",
            "Epoch 12/20\n",
            "9/9 [==============================] - 165s 19s/step - loss: 0.2883 - accuracy: 0.9504 - val_loss: 1.1764 - val_accuracy: 0.8592\n",
            "Epoch 13/20\n",
            "9/9 [==============================] - 170s 19s/step - loss: 0.9397 - accuracy: 0.8676 - val_loss: 0.6602 - val_accuracy: 0.8592\n",
            "Epoch 14/20\n",
            "9/9 [==============================] - 177s 20s/step - loss: 0.4023 - accuracy: 0.8791 - val_loss: 0.3381 - val_accuracy: 0.9399\n",
            "Epoch 15/20\n",
            "9/9 [==============================] - 178s 20s/step - loss: 0.2771 - accuracy: 0.9190 - val_loss: 0.2441 - val_accuracy: 0.8807\n",
            "Epoch 16/20\n",
            "9/9 [==============================] - 178s 20s/step - loss: 0.1919 - accuracy: 0.9118 - val_loss: 0.1683 - val_accuracy: 0.9354\n",
            "Epoch 17/20\n",
            "9/9 [==============================] - 163s 18s/step - loss: 0.1244 - accuracy: 0.9623 - val_loss: 0.1173 - val_accuracy: 0.9686\n",
            "Epoch 18/20\n",
            "9/9 [==============================] - 161s 18s/step - loss: 0.0768 - accuracy: 0.9834 - val_loss: 0.0832 - val_accuracy: 0.9767\n",
            "Epoch 19/20\n",
            "9/9 [==============================] - 161s 18s/step - loss: 0.0455 - accuracy: 0.9883 - val_loss: 0.0710 - val_accuracy: 0.9794\n",
            "Epoch 20/20\n",
            "9/9 [==============================] - 156s 17s/step - loss: 0.0332 - accuracy: 0.9906 - val_loss: 0.0655 - val_accuracy: 0.9803\n",
            "35/35 [==============================] - 17s 464ms/step\n",
            "Precision: 1.0\n",
            "Recall: 0.8598726114649682\n",
            "F1-score: 0.9246575342465754\n"
          ]
        }
      ]
    }
  ]
}